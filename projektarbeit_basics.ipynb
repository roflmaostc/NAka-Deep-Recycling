{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2c4cf6",
   "metadata": {},
   "source": [
    "# Kaggle Garbage Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45755df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU usage\n",
    "use_gpu = True\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Specify data directory\n",
    "data_root = '../../datasets/kaggle-garbage-classification/'\n",
    "data_dir = os.path.join(data_root, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156b1eb",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0852a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "\"\"\"\n",
    "# For FCN\n",
    "trans = transforms.Compose([\n",
    "                transforms.Resize((48, 64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.6718, 0.6381, 0.6041],\n",
    "                                     [0.2005, 0.2032, 0.2268])\n",
    "            ])\n",
    "# For CNN\n",
    "trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.6718, 0.6381, 0.6041],\n",
    "                                     [0.2005, 0.2032, 0.2268])\n",
    "            ])\n",
    "\"\"\"\n",
    "trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.6718, 0.6381, 0.6041],\n",
    "                                     [0.2005, 0.2032, 0.2268])\n",
    "            ])\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform=trans)\n",
    "\n",
    "\n",
    "# Show a sample image\n",
    "sample_img = dataset[567][0].numpy().transpose(1, 2, 0)\n",
    "# Renormalize image to interval [0, 1]\n",
    "mi, ma = (np.min(sample_img, axis=(0, 1)), np.max(sample_img, axis=(0, 1)))\n",
    "sample_img = (sample_img - mi) / (ma - mi)\n",
    "plt.imshow(sample_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e291325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subsets form list\n",
    "splits = ['train', 'val', 'test']\n",
    "data_splits = []\n",
    "for s in splits:\n",
    "    list_name_class = pd.read_csv(os.path.join(data_root, 'one-indexed-files-notrash_%s.txt' % s), sep=' ', header=None)\n",
    "    names = np.array(list_name_class[0])\n",
    "    names = np.sort(names)\n",
    "\n",
    "    inds = []\n",
    "    j = 0\n",
    "    for i, img in enumerate(dataset.imgs):\n",
    "        file_name = os.path.split(img[0])[-1]\n",
    "        if file_name == names[j]:\n",
    "            inds.append(i)\n",
    "            j = j + 1\n",
    "        if j >= len(names):\n",
    "            break\n",
    "    data_splits.append(torch.utils.data.Subset(dataset, inds))\n",
    "\n",
    "trainset, valset, testset = data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731ebe2",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, channels= [9216] + 5 * [128], \n",
    "                 n_classes=6):\n",
    "        # Initialize object\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup parameters\n",
    "        self.n_layers = len(channels) - 1\n",
    "        \n",
    "        # Compose layers\n",
    "        layers = [nn.Flatten()]\n",
    "        for i in range(self.n_layers):\n",
    "            layers.append(nn.Linear(channels[i], channels[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(channels[-1], n_classes))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35404cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, channels=[8, 16, 32, 64, 128, 258, 512], \n",
    "                 kernel_size=3, strides=[2, 2, 2, 2, 2, 2, 2],\n",
    "                 final_size=(3, 4), n_classes=6):\n",
    "        # Initialize object\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup parameters\n",
    "        self.n_layers = len(channels)\n",
    "        channels = [3] + channels\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = self.n_layers * [kernel_size]\n",
    "        \n",
    "        # Compose layers\n",
    "        layers = []\n",
    "        for i in range(self.n_layers):\n",
    "            layers.append(nn.Conv2d(channels[i], channels[i + 1],\n",
    "                                    kernel_size[i], strides[i], padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        \n",
    "        # Average pooling, flatten, and linear layer at the end\n",
    "        layers.append(nn.AvgPool2d(final_size))\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(channels[-1], n_classes))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60023e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "#model = FullyConnectedNeuralNetwork()\n",
    "model = ConvolutionalNeuralNetwork()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d071ca",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "batch_size = 32\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size)\n",
    "\n",
    "# Loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Scheduler for learning rate decay\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.6, patience=3)\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    n = 0\n",
    "    loss_cum = 0\n",
    "    acc_cum = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = (x.to(device), y.to(device))\n",
    "            batch_size = len(x)\n",
    "            pred = model(x)\n",
    "            loss_cum = loss_cum + loss_fn(pred, y).item() * batch_size\n",
    "            pred_class = torch.argmax(pred, -1)\n",
    "            acc_cum = acc_cum + torch.sum(pred_class == y).item()\n",
    "            n = n + batch_size\n",
    "    return loss_cum / n, acc_cum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af8657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do Training\n",
    "n_epoch = 2\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    # Run training epoch\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = (x.to(device), y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        train_loss_hist.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    val_perf = evaluate(model, val_loader, loss_fn, device)\n",
    "    print('Epoch: %i, Valiation loss: %f, Validation accuracy: %f' % ((i + 1,) + val_perf))\n",
    "    val_loss_hist.append(val_perf[0])\n",
    "    val_acc_hist.append(val_perf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7628186",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = np.linspace(0, n_epoch, len(train_loss_hist) + 1)[1:]\n",
    "ep = np.arange(n_epoch) + 1\n",
    "\n",
    "plt.plot(it, train_loss_hist)\n",
    "plt.plot(ep, val_loss_hist)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ep, val_acc_hist)\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afd007",
   "metadata": {},
   "source": [
    "## Evaluate on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss and accuracy on testset\n",
    "test_perf = evaluate(model, test_loader, loss_fn, device)\n",
    "print('Test loss: %f, Test accuracy: %f' % test_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1731e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix \n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        pred = model(x.to(device))\n",
    "        pred_class = torch.argmax(pred, -1)\n",
    "        predictions.extend(pred_class.cpu().numpy())\n",
    "        labels.extend(y.numpy())\n",
    "\n",
    "cm = confusion_matrix(labels, predictions, normalize='true')\n",
    "cm_pd = pd.DataFrame(cm, dataset.classes, dataset.classes)\n",
    "\n",
    "sn.heatmap(cm_pd, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

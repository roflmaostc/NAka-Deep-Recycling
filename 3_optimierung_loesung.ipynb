{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8e6407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0943872",
   "metadata": {},
   "source": [
    "Wir haben in den Referaten bereits einiges über Optimierung gehört.\n",
    "\n",
    "Nun ist es Zeit für praktische Übungen.\n",
    "Letztendlich ist Machine Learning ein großes Optimierungsproblem wo man z.B. versucht die Parameters des Netzwerks an Trainingsdaten so anzupassen, dass diese einen möglichst optimalen Output erzeugen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b286c8",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent\n",
    "\n",
    "Gradient Descent wird auch der Weg des steilsten Abstieges genannt.\n",
    "Ein (lokales) Minimum lässt sich damit finden, indem man ähnlich wie bei einer Wanderung einfach steil bergab geht.\n",
    "Die Richtung des Abstieges lässt sich mit der/dem Ableitung/Gradient bestimmen.\n",
    "\n",
    "\n",
    "Gradient Descent lässt sich formal schreiben für eine Funktion $f:\\mathbb{R}\\mapsto \\mathbb{R}$:\n",
    "\n",
    "$$ x_{n+1} = x_n - \\gamma \\cdot \\frac{\\mathrm{d}f}{\\mathrm{d}x}\\bigg\\vert_{x_n} = x_n - \\gamma \\cdot f'(x_n)$$\n",
    "\n",
    "In Falle von mehrerer Variablen (z.B. als Vektor gespeichert), gilt für $h:\\mathbb{R}^N\\mapsto \\mathbb{R}^N$\n",
    "\n",
    "$$ \\vec x_{n+1} = \\vec x_n - \\gamma \\cdot (\\nabla h)(\\vec x_n)$$\n",
    "\n",
    "In beiden Fällen ist $\\gamma$ die Schrittweite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "971573ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def gradient_descent(df, x0, N, step_size=1e-2):\n",
    "    \"\"\"Returns the optimum value for x\n",
    "\n",
    "    Arguments:\n",
    "    df: A function which evaluates the derivative/gradient of f\n",
    "    x0: Start value\n",
    "    N: number of iterations\n",
    "    step_size: Step size in each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # aktuelle Position\n",
    "    x = x0\n",
    "    \n",
    "    l = [x0]\n",
    "\n",
    "    for i in range(N):\n",
    "        # hier gucken rum und finden steilen Abstieg\n",
    "        gradient = df(x)\n",
    "        # hier gehen wir den Schritt\n",
    "        x = x - step_size * gradient\n",
    "        l.append(x)\n",
    "\n",
    "    # gibt das Ergebnis und alle Zwischenwerte zurück\n",
    "    return x, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed9e10",
   "metadata": {},
   "source": [
    "## Beispiel 1.1 Gradient Descent\n",
    "Implementiere nun eine Funktion und deren Ableitung. Überprüfe, ob der gradient descent Mechanismus funktioniert.\n",
    "\n",
    "Welche Funktion eignet sich?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0389875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x-3)**4\n",
    "    \n",
    "# Ableitung von f\n",
    "def df(x):\n",
    "    return 4 * (x-3)**3 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e512617",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loesung, small_step = gradient_descent(df, 0, 1000, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "490cd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loesung, big_step = gradient_descent(df, 0, 1000, step_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54bee89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff1bc50a380>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(small_step)\n",
    "plt.plot(big_step)\n",
    "plt.legend([\"Small Step\", \"Big Step\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1c920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3879d182",
   "metadata": {},
   "source": [
    "## Beispiel 1.2 Gradient Descent\n",
    "\n",
    "Implementiere eine Funktion mit mehreren Variablen (z.B. $(x - 21.3) ^2 + (y + 13.1)^4$).\n",
    "\n",
    "Geht die Implementierung von oben nun auch für den mehrdimensionalen Fall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, Funktion \n",
    "def f_2(x):\n",
    "    return 0\n",
    "    \n",
    "# TODO, Ableitung der Funktion\n",
    "def df_2(x):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63299e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(df_2, np.array([0.0, 0.0]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfd269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4a1ba7",
   "metadata": {},
   "source": [
    "## Beispiel 1.3\n",
    "\n",
    "Implementiere eine gradient descent Funktion, welche sowohl den Funktionswert, als auch $x_n$ in jeder Iteration speichert.\n",
    "Nutze dafür ein array oder eine Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_track(f, df, x0, N, step_size=1e-2):\n",
    "    \n",
    "    return x0, log, log_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40104045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0, log, log_f = gradient_descent_track(f_2, df_2, np.array([0.0, 0.0]), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf79fdf",
   "metadata": {},
   "source": [
    "## Beispiel 1.4\n",
    "\n",
    "Plotte den Funktionswert von `f_2` über die Anzahl der Iterationen.\n",
    "Plotte in ein neues Fenster die Werte der Variablen über die Anzahl der Iterationen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773eedfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db4793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6102c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b978f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69598f8a",
   "metadata": {},
   "source": [
    "# 2. Automatic Differentation\n",
    "\n",
    "Bisher war es recht mühsam, da wir die Ableitung per Hand ausgerechnet haben.\n",
    "Zum Glück müssen wir das bei unseren Neuronalen Netzen nicht machen, da PyTorch Mechanismen hierfür eingebaut hat (Automatic Differentation).\n",
    "\n",
    "Wir wollen nun komplexere Funktionen optimieren, ohne den Gradient per Hand zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033468bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea17c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegeben ist folgende Funktion g\n",
    "def g_torch(x):\n",
    "    return (x[0]-1)**2  + (torch.sqrt(x[1]) - torch.log(x[2]))**2 + x[1]**2 + (torch.log(x[1]) - 2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264a732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fcbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_track_ad(f, x0, N, step_size=1e-2):\n",
    "    # TODO\n",
    "    # hier fehlen ein paar Variablen\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        l = 0 # TODO, was könnte hier wohl stehen?\n",
    "        \n",
    "        # damit wird der gradient berechnet\n",
    "        l.backward()\n",
    "        df = x.grad\n",
    "        # spezielles construct\n",
    "        with torch.no_grad():\n",
    "            # step update\n",
    "            x = 0 # TODO\n",
    "            \n",
    "            # spezielles construct \n",
    "            x = torch.tensor(x, requires_grad=True)\n",
    "            \n",
    "            # speichere hier noch die variablen\n",
    "            # TODO\n",
    "            \n",
    "    return x0, log, log_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_log, t_log_f = gradient_descent_track_ad(f_2, torch.tensor([1.0, 1.0], requires_grad=True), \n",
    "                                                100, \n",
    "                                                step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626aad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(t_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fab8ea",
   "metadata": {},
   "source": [
    "# 3. Optimierer von PyTorch\n",
    "\n",
    "PyTorch stellt eine ganze Reihe von verschiedenen Optimierern zur Verfügung.\n",
    "`ADAM` wird relativ häufig benutzt.\n",
    "\n",
    "Teste nun verschiedene Arten von Optimierern. Versuche auch eine graphische Darstellung zu machen, in denen du verschiedene Optimierer miteinander hinsichtlich Konvergenz und Startbedingungen vergleichst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9377d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orientiere dich an der vorherigen Funktion gradient_descent_track_ad\n",
    "# Im Prinzip müssen nur wenige Teile geändert werden\n",
    "def ADAM_optimize(f, x0, N, step_size=1e-2):\n",
    "    # TODO\n",
    "    \n",
    "    optimizer = torch.optim.Adam([x], lr=0.1) # initialisiere hier den optimizer\n",
    "    for i in range(N):\n",
    "        optimizer.zero_grad()\n",
    "        l = f(x)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # speichere hier noch die log values, dies muss wieder innerhalb des `with torch.no_grad():` passieren\n",
    " \n",
    "      \n",
    "            \n",
    "    return x, log, log_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cdb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e120e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiere die Funktion nun so, dass ein bestimmter optimierer über das keyword argument\n",
    "# optimizer übergeben werden kann\n",
    "def optimize(f, x0, N, optimizer=torch.optim.SGD, lr=1e-2):\n",
    "    # ähnlich\n",
    "            \n",
    "    return x, log, log_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d7643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64927626",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_log, t_log_f = ADAM_optimize(f_2, torch.tensor([0.0, 0.0], requires_grad=True), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f18c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x2, t_log2, t_log_f2 = optimize(f_2, torch.tensor([0.0, 0.0], requires_grad=True), 1000,\\\n",
    "                                        optimizer=torch.optim.Adadelta, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249758a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(t_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb245a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2491e921",
   "metadata": {},
   "source": [
    "# 4. Rosenbrock Funktion\n",
    "\n",
    "Mache auch die gleichen Vergleiche auch mit der Rosenbrock Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8af282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return (1.0 - x[0])**2 + 100.0 * (x[1] - x[0]**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71002",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_2, t_log_2, t_log_f_2 = torch_optimize(rosenbrock, torch.tensor([0.0, 0.0], requires_grad=True), 500,  lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139598b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(t_log_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934a8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbb6642",
   "metadata": {},
   "source": [
    "# 5. Weitere Funktionen\n",
    "\n",
    "Auf Wikipedia findet man eine Reihe ganz verschiedener Funktionen.\n",
    "\n",
    "Versuche ein paar davon zu optimieren!\n",
    "\n",
    "https://en.wikipedia.org/wiki/Test_functions_for_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633c0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_log, t_log_f = torch_optimize(      , torch.tensor([0.2, 0.2], requires_grad=True), 5000, lr =0.01,\\\n",
    "                                     optimizer=torch.optim.Adadelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0a9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
